{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to image folders\n",
    "train_low_folder = \"/home/dgreen/sem2/dl/finalProject/LOL-v2/LOL-v2/Real_captured/Train/Low\"  # Path to folder containing low-resolution training images\n",
    "train_high_folder = \"/home/dgreen/sem2/dl/finalProject/LOL-v2/LOL-v2/Real_captured/Train/Normal\"  # Path to folder containing high-resolution training images\n",
    "eval_low_folder = \"/home/dgreen/sem2/dl/finalProject/LOL-v2/LOL-v2/Real_captured/Test/Low\"  # Path to folder containing low-resolution evaluation images\n",
    "eval_high_folder = \"/home/dgreen/sem2/dl/finalProject/LOL-v2/LOL-v2/Real_captured/Test/Normal\"  # Path to folder containing high-resolution evaluation images\n",
    "train_low_synt_folder = \"/home/dgreen/sem2/dl/finalProject/LOL-v2/LOL-v2/Synthetic/Train/Low\"\n",
    "train_high_synt_folder = \"/home/dgreen/sem2/dl/finalProject/LOL-v2/LOL-v2/Synthetic/Train/Normal\"\n",
    "eval_low_synt_folder = \"/home/dgreen/sem2/dl/finalProject/LOL-v2/LOL-v2/Synthetic/Test/Low\"\n",
    "eval_high_synt_folder = \"/home/dgreen/sem2/dl/finalProject/LOL-v2/LOL-v2/Synthetic/Test/Normal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_low_images_list = []\n",
    "train_high_images_list = []\n",
    "eval_low_images_list = []\n",
    "eval_high_images_list = []\n",
    "\n",
    "train_low_synt_images_list = []\n",
    "train_high_synt_images_list = []\n",
    "eval_low_synt_images_list = []\n",
    "eval_high_synt_images_list = []\n",
    "\n",
    "for filename in os.listdir(train_low_folder):\n",
    "    if \"png\" in filename.strip().split(\".\")[-1]:\n",
    "        train_low_images_list.append(filename)\n",
    "\n",
    "for filename in os.listdir(train_high_folder):\n",
    "    if \"png\" in filename.strip().split(\".\")[-1]:\n",
    "        train_high_images_list.append(filename)\n",
    "\n",
    "for filename in os.listdir(eval_low_folder):\n",
    "    if \"png\" in filename.strip().split(\".\")[-1]:\n",
    "        eval_low_images_list.append(filename)\n",
    "\n",
    "for filename in os.listdir(eval_high_folder):\n",
    "    if \"png\" in filename.strip().split(\".\")[-1]:\n",
    "        eval_high_images_list.append(filename)\n",
    "\n",
    "for filename in os.listdir(train_low_synt_folder):\n",
    "    if \"png\" in filename.strip().split(\".\")[-1]:\n",
    "        train_low_synt_images_list.append(filename)\n",
    "\n",
    "for filename in os.listdir(train_high_synt_folder):\n",
    "    if \"png\" in filename.strip().split(\".\")[-1]:\n",
    "        train_high_synt_images_list.append(filename)\n",
    "\n",
    "for filename in os.listdir(eval_low_synt_folder):\n",
    "    if \"png\" in filename.strip().split(\".\")[-1]:\n",
    "        eval_low_synt_images_list.append(filename)\n",
    "\n",
    "for filename in os.listdir(eval_high_synt_folder):\n",
    "    if \"png\" in filename.strip().split(\".\")[-1]:\n",
    "        eval_high_synt_images_list.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_low_images_list = sorted(train_low_images_list)\n",
    "train_high_images_list = sorted(train_high_images_list)\n",
    "eval_low_images_list = sorted(eval_low_images_list)\n",
    "eval_high_images_list = sorted(eval_high_images_list)\n",
    "\n",
    "train_low_synt_images_list = sorted(train_low_synt_images_list)\n",
    "train_high_synt_images_list = sorted(train_high_synt_images_list)\n",
    "eval_low_synt_images_list = sorted(eval_low_synt_images_list)\n",
    "eval_high_synt_images_list = sorted(eval_high_synt_images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load low-resolution and high-resolution images using OpenCV\n",
    "train_low_images = [cv2.imread(os.path.join(train_low_folder, filename)) for filename in train_low_images_list]\n",
    "train_high_images = [cv2.imread(os.path.join(train_high_folder, filename)) for filename in train_high_images_list]\n",
    "eval_low_images = [cv2.imread(os.path.join(eval_low_folder, filename)) for filename in eval_low_images_list]\n",
    "eval_high_images = [cv2.imread(os.path.join(eval_high_folder, filename)) for filename in eval_high_images_list]\n",
    "\n",
    "train_low_images_fld_list = [os.path.join(train_low_folder, filename) for filename in train_low_images_list]\n",
    "train_high_images_fld_list = [os.path.join(train_high_folder, filename) for filename in train_high_images_list]\n",
    "eval_low_images_fld_list = [os.path.join(eval_low_folder, filename) for filename in eval_low_images_list]\n",
    "eval_high_images_fld_list = [os.path.join(eval_high_folder, filename) for filename in eval_high_images_list]\n",
    "\n",
    "train_low_synt_images = [cv2.imread(os.path.join(train_low_synt_folder, filename)) for filename in train_low_synt_images_list]\n",
    "train_high_synt_images = [cv2.imread(os.path.join(train_high_synt_folder, filename)) for filename in train_high_synt_images_list]\n",
    "eval_low_synt_images = [cv2.imread(os.path.join(eval_low_synt_folder, filename)) for filename in eval_low_synt_images_list]\n",
    "eval_high_synt_images = [cv2.imread(os.path.join(eval_high_synt_folder, filename)) for filename in eval_high_synt_images_list]\n",
    "\n",
    "train_low_synt_images_fld_list = [os.path.join(train_low_synt_folder, filename) for filename in train_low_synt_images_list]\n",
    "train_high_synt_images_fld_list = [os.path.join(train_high_synt_folder, filename) for filename in train_high_synt_images_list]\n",
    "eval_low_synt_images_fld_list = [os.path.join(eval_low_synt_folder, filename) for filename in eval_low_synt_images_list]\n",
    "eval_high_synt_images_fld_list = [os.path.join(eval_high_synt_folder, filename) for filename in eval_high_synt_images_list]\n",
    "\n",
    "train_low_images = train_low_images + train_low_synt_images\n",
    "train_high_images = train_high_images + train_high_synt_images\n",
    "eval_low_images = eval_low_images + eval_low_synt_images\n",
    "eval_high_images = eval_high_images + eval_high_synt_images\n",
    "\n",
    "train_low_images_fld_list = train_low_images_fld_list + train_low_synt_images_fld_list\n",
    "train_high_images_fld_list = train_high_images_fld_list + train_high_synt_images_fld_list\n",
    "eval_low_images_fld_list = eval_low_images_fld_list + eval_low_synt_images_fld_list\n",
    "eval_high_images_fld_list = eval_high_images_fld_list + eval_high_synt_images_fld_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, low_res_images, high_res_images, transform=None, resize=(384, 384)):\n",
    "        self.low_res_images = low_res_images\n",
    "        self.high_res_images = high_res_images\n",
    "        self.transform = transform\n",
    "        self.resize = resize\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.low_res_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            low_res_image = Image.open(self.low_res_images[idx]).convert('RGB')\n",
    "            high_res_image = Image.open(self.high_res_images[idx]).convert('RGB')\n",
    "\n",
    "            # Resize images\n",
    "            low_res_image = low_res_image.resize(self.resize, Image.BICUBIC)\n",
    "            high_res_image = high_res_image.resize(self.resize, Image.BICUBIC)\n",
    "            \n",
    "            if self.transform:\n",
    "                low_res_image = self.transform(low_res_image)\n",
    "                high_res_image = self.transform(high_res_image)\n",
    "            \n",
    "            return low_res_image, high_res_image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image at index {idx}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "# Define transformation for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = CustomDataset(train_low_images_fld_list, train_high_images_fld_list, transform=transform)\n",
    "eval_dataset = CustomDataset(eval_low_images_fld_list, eval_high_images_fld_list, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(eval_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.batchnorm = nn.BatchNorm2d(channels)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x += residual\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.residual_blocks = nn.ModuleList([ResidualBlock(64) for _ in range(8)])\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation(x)\n",
    "        for block in self.residual_blocks:\n",
    "            x = block(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models moved to GPU.\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from torch import nn, optim\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# # -----------------------------------\n",
    "# # 1. Generator and Residual Block\n",
    "# # -----------------------------------\n",
    "# class ResidualBlock(nn.Module):\n",
    "#     def __init__(self, in_features):\n",
    "#         super(ResidualBlock, self).__init__()\n",
    "#         self.block = nn.Sequential(\n",
    "#             nn.ReflectionPad2d(1),\n",
    "#             nn.Conv2d(in_features, in_features, 3),\n",
    "#             nn.InstanceNorm2d(in_features),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.ReflectionPad2d(1),\n",
    "#             nn.Conv2d(in_features, in_features, 3),\n",
    "#             nn.InstanceNorm2d(in_features)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x + self.block(x)\n",
    "\n",
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, input_channels, output_channels, n_residual_blocks=9):\n",
    "#         super(Generator, self).__init__()\n",
    "#         model = [nn.ReflectionPad2d(3),\n",
    "#                  nn.Conv2d(input_channels, 64, 7),\n",
    "#                  nn.InstanceNorm2d(64),\n",
    "#                  nn.ReLU(inplace=True)]\n",
    "#         in_features = 64\n",
    "#         out_features = in_features * 2\n",
    "#         for _ in range(2):\n",
    "#             model.extend([nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "#                           nn.InstanceNorm2d(out_features),\n",
    "#                           nn.ReLU(inplace=True)])\n",
    "#             in_features = out_features\n",
    "#             out_features *= 2\n",
    "#         for _ in range(n_residual_blocks):\n",
    "#             model.append(ResidualBlock(in_features))\n",
    "#         out_features = in_features // 2\n",
    "#         for _ in range(2):\n",
    "#             model.extend([nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "#                           nn.InstanceNorm2d(out_features),\n",
    "#                           nn.ReLU(inplace=True)])\n",
    "#             in_features = out_features\n",
    "#             out_features = in_features // 2\n",
    "#         model.extend([nn.ReflectionPad2d(3),\n",
    "#                       nn.Conv2d(64, output_channels, 7),\n",
    "#                       nn.Tanh()])\n",
    "#         self.model = nn.Sequential(*model)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "# # -----------------------------------\n",
    "# # 2. Discriminator\n",
    "# # -----------------------------------\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, input_channels):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Conv2d(input_channels, 64, 4, stride=2, padding=1),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "#             nn.InstanceNorm2d(128),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Conv2d(128, 256, 4, stride=2, padding=1),\n",
    "#             nn.InstanceNorm2d(256),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Conv2d(256, 512, 4, padding=1),\n",
    "#             nn.InstanceNorm2d(512),\n",
    "#             nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Conv2d(512, 1, 4, padding=1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "# # -----------------------------------\n",
    "# # 3. Initialize Models\n",
    "# # -----------------------------------\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# G_AB = Generator(3, 3).to(device)\n",
    "# G_BA = Generator(3, 3).to(device)\n",
    "# D_A = Discriminator(3).to(device)\n",
    "# D_B = Discriminator(3).to(device)\n",
    "# print('Models moved to GPU.')\n",
    "\n",
    "# # -----------------------------------\n",
    "# # 4. Losses and Optimizers\n",
    "# # -----------------------------------\n",
    "# criterion_GAN = nn.MSELoss()\n",
    "# criterion_cycle = nn.L1Loss()\n",
    "# criterion_identity = nn.L1Loss()\n",
    "\n",
    "# optimizer_G = optim.Adam(list(G_AB.parameters()) + list(G_BA.parameters()), lr=0.0002, betas=(0.5, 0.999))\n",
    "# optimizer_D_A = optim.Adam(D_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "# optimizer_D_B = optim.Adam(D_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DiffusionModel:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"batchnorm1.weight\", \"batchnorm1.bias\", \"batchnorm1.running_mean\", \"batchnorm1.running_var\", \"residual_blocks.0.conv1.weight\", \"residual_blocks.0.conv1.bias\", \"residual_blocks.0.batchnorm.weight\", \"residual_blocks.0.batchnorm.bias\", \"residual_blocks.0.batchnorm.running_mean\", \"residual_blocks.0.batchnorm.running_var\", \"residual_blocks.1.conv1.weight\", \"residual_blocks.1.conv1.bias\", \"residual_blocks.1.batchnorm.weight\", \"residual_blocks.1.batchnorm.bias\", \"residual_blocks.1.batchnorm.running_mean\", \"residual_blocks.1.batchnorm.running_var\", \"residual_blocks.2.conv1.weight\", \"residual_blocks.2.conv1.bias\", \"residual_blocks.2.batchnorm.weight\", \"residual_blocks.2.batchnorm.bias\", \"residual_blocks.2.batchnorm.running_mean\", \"residual_blocks.2.batchnorm.running_var\", \"residual_blocks.3.conv1.weight\", \"residual_blocks.3.conv1.bias\", \"residual_blocks.3.batchnorm.weight\", \"residual_blocks.3.batchnorm.bias\", \"residual_blocks.3.batchnorm.running_mean\", \"residual_blocks.3.batchnorm.running_var\", \"residual_blocks.4.conv1.weight\", \"residual_blocks.4.conv1.bias\", \"residual_blocks.4.batchnorm.weight\", \"residual_blocks.4.batchnorm.bias\", \"residual_blocks.4.batchnorm.running_mean\", \"residual_blocks.4.batchnorm.running_var\", \"residual_blocks.5.conv1.weight\", \"residual_blocks.5.conv1.bias\", \"residual_blocks.5.batchnorm.weight\", \"residual_blocks.5.batchnorm.bias\", \"residual_blocks.5.batchnorm.running_mean\", \"residual_blocks.5.batchnorm.running_var\", \"residual_blocks.6.conv1.weight\", \"residual_blocks.6.conv1.bias\", \"residual_blocks.6.batchnorm.weight\", \"residual_blocks.6.batchnorm.bias\", \"residual_blocks.6.batchnorm.running_mean\", \"residual_blocks.6.batchnorm.running_var\", \"residual_blocks.7.conv1.weight\", \"residual_blocks.7.conv1.bias\", \"residual_blocks.7.batchnorm.weight\", \"residual_blocks.7.batchnorm.bias\", \"residual_blocks.7.batchnorm.running_mean\", \"residual_blocks.7.batchnorm.running_var\", \"conv2.weight\", \"conv2.bias\", \"batchnorm2.weight\", \"batchnorm2.bias\", \"batchnorm2.running_mean\", \"batchnorm2.running_var\", \"conv3.weight\", \"conv3.bias\". \n\tUnexpected key(s) in state_dict: \"encoder_conv1.weight\", \"encoder_conv1.bias\", \"encoder_conv2.weight\", \"encoder_conv2.bias\", \"decoder_conv1.weight\", \"decoder_conv1.bias\", \"decoder_conv2.weight\", \"decoder_conv2.bias\", \"decoder_conv3.weight\", \"decoder_conv3.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m model2\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiffusion_model_100.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     24\u001b[0m model3 \u001b[38;5;241m=\u001b[39m DiffusionModel()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mmodel3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiffusion_model_actual.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m model4 \u001b[38;5;241m=\u001b[39m Generator(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     27\u001b[0m model4\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mG_AB_epoch_100.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DiffusionModel:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"batchnorm1.weight\", \"batchnorm1.bias\", \"batchnorm1.running_mean\", \"batchnorm1.running_var\", \"residual_blocks.0.conv1.weight\", \"residual_blocks.0.conv1.bias\", \"residual_blocks.0.batchnorm.weight\", \"residual_blocks.0.batchnorm.bias\", \"residual_blocks.0.batchnorm.running_mean\", \"residual_blocks.0.batchnorm.running_var\", \"residual_blocks.1.conv1.weight\", \"residual_blocks.1.conv1.bias\", \"residual_blocks.1.batchnorm.weight\", \"residual_blocks.1.batchnorm.bias\", \"residual_blocks.1.batchnorm.running_mean\", \"residual_blocks.1.batchnorm.running_var\", \"residual_blocks.2.conv1.weight\", \"residual_blocks.2.conv1.bias\", \"residual_blocks.2.batchnorm.weight\", \"residual_blocks.2.batchnorm.bias\", \"residual_blocks.2.batchnorm.running_mean\", \"residual_blocks.2.batchnorm.running_var\", \"residual_blocks.3.conv1.weight\", \"residual_blocks.3.conv1.bias\", \"residual_blocks.3.batchnorm.weight\", \"residual_blocks.3.batchnorm.bias\", \"residual_blocks.3.batchnorm.running_mean\", \"residual_blocks.3.batchnorm.running_var\", \"residual_blocks.4.conv1.weight\", \"residual_blocks.4.conv1.bias\", \"residual_blocks.4.batchnorm.weight\", \"residual_blocks.4.batchnorm.bias\", \"residual_blocks.4.batchnorm.running_mean\", \"residual_blocks.4.batchnorm.running_var\", \"residual_blocks.5.conv1.weight\", \"residual_blocks.5.conv1.bias\", \"residual_blocks.5.batchnorm.weight\", \"residual_blocks.5.batchnorm.bias\", \"residual_blocks.5.batchnorm.running_mean\", \"residual_blocks.5.batchnorm.running_var\", \"residual_blocks.6.conv1.weight\", \"residual_blocks.6.conv1.bias\", \"residual_blocks.6.batchnorm.weight\", \"residual_blocks.6.batchnorm.bias\", \"residual_blocks.6.batchnorm.running_mean\", \"residual_blocks.6.batchnorm.running_var\", \"residual_blocks.7.conv1.weight\", \"residual_blocks.7.conv1.bias\", \"residual_blocks.7.batchnorm.weight\", \"residual_blocks.7.batchnorm.bias\", \"residual_blocks.7.batchnorm.running_mean\", \"residual_blocks.7.batchnorm.running_var\", \"conv2.weight\", \"conv2.bias\", \"batchnorm2.weight\", \"batchnorm2.bias\", \"batchnorm2.running_mean\", \"batchnorm2.running_var\", \"conv3.weight\", \"conv3.bias\". \n\tUnexpected key(s) in state_dict: \"encoder_conv1.weight\", \"encoder_conv1.bias\", \"encoder_conv2.weight\", \"encoder_conv2.bias\", \"decoder_conv1.weight\", \"decoder_conv1.bias\", \"decoder_conv2.weight\", \"decoder_conv2.bias\", \"decoder_conv3.weight\", \"decoder_conv3.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import exposure\n",
    "\n",
    "# Define PSNR calculation function\n",
    "def psnr(img1, img2):\n",
    "    mse = torch.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
    "\n",
    "\n",
    "# Load the models\n",
    "model1 = DiffusionModel().to(device)\n",
    "model1.load_state_dict(torch.load('diffusion_model.pth'))\n",
    "# model2 = DiffusionModel().to(device)\n",
    "# model2.load_state_dict(torch.load('diffusion_model_100.pth'))\n",
    "# model3 = DiffusionModel().to(device)\n",
    "# model3.load_state_dict(torch.load('diffusion_model_actual.pth'))\n",
    "# model4 = Generator(3, 3)\n",
    "# model4.load_state_dict(torch.load('G_AB_epoch_100.pth'))\n",
    "# model5 = Generator(3, 3)\n",
    "# model5.load_state_dict(torch.load('G_AB_200epoch_200.pth'))\n",
    "\n",
    "# Load the test dataset\n",
    "\n",
    "\n",
    "# Set models to evaluation mode\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "model3.eval()\n",
    "model4.eval()\n",
    "model5.eval()\n",
    "\n",
    "def adjust_color_balance(image):\n",
    "    # Example: Apply a simple color balance adjustment by increasing the red channel\n",
    "    image[:,:,:] *= 1.25  # Increase red channel\n",
    "\n",
    "    # Clip values to ensure they are in the valid range [0, 1]\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Iterate through the test dataset\n",
    "for i, (low_img, high_img) in enumerate(test_loader):\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the models\n",
    "        output1 = model1(low_img)\n",
    "        output2 = model2(low_img)\n",
    "        output3 = model3(low_img)\n",
    "        output4 = model4(low_img)\n",
    "        output5 = model5(low_img)\n",
    "        # Calculate PSNR for each output\n",
    "        psnr1 = psnr(output1, high_img)\n",
    "        psnr2 = psnr(output2, high_img)\n",
    "        psnr3 = psnr(output3, high_img)\n",
    "        psnr4 = psnr(output4, high_img)\n",
    "        psnr5 = psnr(output5, high_img)\n",
    "\n",
    "        # Print PSNR scores\n",
    "        print(f\"PSNR for Model 1 on image {i+1}: {psnr1.item()} dB\")\n",
    "        print(f\"PSNR for Model 2 on image {i+1}: {psnr2.item()} dB\")\n",
    "        print(f\"PSNR for Model 3 on image {i+1}: {psnr3.item()} dB\")\n",
    "        print(f\"PSNR for Model 4 on image {i+1}: {psnr4.item()} dB\")\n",
    "        print(f\"PSNR for Model 5 on image {i+1}: {psnr5.item()} dB\")\n",
    "\n",
    "\n",
    "        normalized_output1 = (output1 - output1.min()) / (output1.max() - output1.min())\n",
    "        output_image1_np = normalized_output1.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "        output_image1_np_adjusted = adjust_color_balance(output_image1_np)\n",
    "        output_image1_np_adjusted = exposure.equalize_hist(output_image1_np_adjusted)\n",
    "\n",
    "        normalized_output2 = (output2 - output2.min()) / (output2.max() - output2.min())\n",
    "        output_image2_np = normalized_output2.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "        output_image2_np_adjusted = adjust_color_balance(output_image2_np)\n",
    "        output_image2_np_adjusted = exposure.equalize_hist(output_image2_np_adjusted)\n",
    "\n",
    "        normalized_output3 = (output3 - output3.min()) / (output3.max() - output3.min())\n",
    "        output_image3_np = normalized_output3.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "        output_image3_np_adjusted = adjust_color_balance(output_image3_np)\n",
    "        output_image3_np_adjusted = exposure.equalize_hist(output_image3_np_adjusted)\n",
    "\n",
    "        normalized_output4 = (output4 - output4.min()) / (output4.max() - output4.min())\n",
    "        output_image4_np = normalized_output4.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "        output_image4_np_adjusted = adjust_color_balance(output_image4_np)\n",
    "        output_image4_np_adjusted = exposure.equalize_hist(output_image4_np_adjusted)\n",
    "        \n",
    "        normalized_output5 = (output5 - output5.min()) / (output5.max() - output5.min())\n",
    "        output_image5_np = normalized_output5.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "        output_image5_np_adjusted = adjust_color_balance(output_image5_np)\n",
    "        output_image5_np_adjusted = exposure.equalize_hist(output_image5_np_adjusted)\n",
    "\n",
    "        # Plot the images\n",
    "        fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "        # Display the first row of images\n",
    "        axes[0, 0].imshow(high_img)\n",
    "        axes[0, 0].set_title('High Resolution')\n",
    "        axes[0, 0].axis('off')\n",
    "\n",
    "        axes[0, 1].imshow(low_img)\n",
    "        axes[0, 1].set_title('Low Resolution')\n",
    "        axes[0, 1].axis('off')\n",
    "\n",
    "        axes[0, 2].imshow(output1)\n",
    "        axes[0, 2].set_title('diffusion_model')\n",
    "        axes[0, 2].axis('off')\n",
    "\n",
    "        # If you have additional images, continue displaying them in the same row\n",
    "        # For example:\n",
    "        axes[0, 3].imshow(output2)\n",
    "        axes[0, 3].set_title('diffusion_model_100')\n",
    "        axes[0, 3].axis('off')\n",
    "\n",
    "        axes[0, 4].imshow(output3)\n",
    "        axes[0, 4].set_title('diffusion_model_actual')\n",
    "        axes[0, 4].axis('off')\n",
    "\n",
    "        axes[0, 5].imshow(output4)\n",
    "        axes[0, 5].set_title('G_AB_epoch_100')\n",
    "        axes[0, 5].axis('off')\n",
    "\n",
    "        axes[0, 5].imshow(output5)\n",
    "        axes[0, 5].set_title('G_AB_200epoch_200')\n",
    "        axes[0, 5].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        if i == 10:  # Change 10 to the number of images you want to display\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
